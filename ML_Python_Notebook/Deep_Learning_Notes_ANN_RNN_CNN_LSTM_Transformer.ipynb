{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9542268f",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Notes: ANN, RNN, CNN, LSTM, and Transformers\n",
    "\n",
    "These in-depth notes cover the core deep learning architectures you’ll use day-to-day—**ANN**, **RNN**, **CNN**, **LSTM**, and **Transformers**—with intuition, equations, design tips, and **ready-to-run PyTorch code**.\n",
    "\n",
    "**What you'll get:**\n",
    "- Intuition and when to use each architecture\n",
    "- Key equations and pitfalls\n",
    "- Minimal but complete PyTorch code templates (CPU-friendly)\n",
    "- Practical tips (initialization, regularization, debugging)\n",
    "\n",
    "> Requirements to run code cells: `torch`, `torchvision`, `numpy`, `matplotlib` (install via `pip install torch torchvision torchaudio numpy matplotlib`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b17915",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Objectives\n",
    "- Understand the building blocks of neural networks and how gradients flow.\n",
    "- Learn when to choose **ANN vs. RNN/LSTM vs. CNN vs. Transformer**.\n",
    "- Implement each model in PyTorch and adapt the templates to your own datasets.\n",
    "- Recognize common training issues (overfitting, vanishing gradients, exploding gradients) and fix them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303566f",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "- Python, NumPy, and basic PyTorch (`nn.Module`, `DataLoader`, `optim`).\n",
    "- Familiarity with gradient descent and loss functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3932b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1) Artificial Neural Networks (ANN / MLP)\n",
    "\n",
    "**Use when:** Data are tabular or features are already engineered (no sequence/image structure).\n",
    "\n",
    "### Intuition\n",
    "- An **MLP** is a stack of linear layers plus nonlinearities.\n",
    "- Nonlinearities (ReLU/GELU/Tanh) allow modeling complex functions.\n",
    "- **Depth** increases expressivity; **width** increases capacity.\n",
    "\n",
    "### Core Equations\n",
    "Given input $\\mathbf{x} \\in \\mathbb{R}^{d}$:\n",
    "- Layer: $\\mathbf{h}^{(l)} = \\sigma(\\mathbf{W}^{(l)}\\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)})$\n",
    "- Output (classification): $\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{W}^{(L)}\\mathbf{h}^{(L-1)} + \\mathbf{b}^{(L)})$\n",
    "- Loss (cross-entropy): $\\mathcal{L} = -\\sum_i y_i \\log \\hat{y}_i$\n",
    "\n",
    "### Notes & Tips\n",
    "- Prefer **ReLU/GELU** over sigmoid/tanh for hidden layers.\n",
    "- Use **BatchNorm/LayerNorm**, **Dropout**, and **weight decay** to regularize.\n",
    "- **Xavier/He** initialization generally works well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321b04b",
   "metadata": {},
   "source": [
    "\n",
    "### Minimal NumPy Forward Pass (for intuition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(4, 3)        # batch=4, features=3\n",
    "W1 = np.random.randn(3, 5) * 0.1 # 3->5\n",
    "b1 = np.zeros(5)\n",
    "W2 = np.random.randn(5, 2) * 0.1 # 5->2\n",
    "b2 = np.zeros(2)\n",
    "\n",
    "def relu(z): \n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# forward\n",
    "h1 = relu(X @ W1 + b1)\n",
    "logits = h1 @ W2 + b2\n",
    "probs = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c963af4",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch: MLP Classifier Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims=(128, 64), out_dim=2, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(p_drop)]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# dummy data (replace with your tensors)\n",
    "X = torch.randn(512, 20)\n",
    "y = torch.randint(0, 2, (512,))\n",
    "\n",
    "ds = TensorDataset(X, y)\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "model = MLP(in_dim=20, out_dim=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in dl:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    print(f\"epoch {epoch+1}: loss={total_loss/len(ds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d3b08",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2) Recurrent Neural Networks (RNN)\n",
    "\n",
    "**Use when:** Data are **sequences** (text, time-series) and dependencies are mostly local/short.\n",
    "\n",
    "### Intuition\n",
    "- RNNs process tokens **sequentially**, carrying a hidden state.\n",
    "- Struggle with **long-range dependencies** due to **vanishing/exploding gradients**.\n",
    "\n",
    "### Equations (Elman RNN)\n",
    "- $\\mathbf{h}_t = \\tanh(\\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h)$\n",
    "- $\\hat{\\mathbf{y}}_t = \\mathrm{softmax}(\\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y)$\n",
    "\n",
    "### Tips\n",
    "- Use gradient clipping; try **LSTM/GRU** if RNN underfits long contexts.\n",
    "- Pack sequences with `pack_padded_sequence` when lengths vary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe095c08",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch: Character-level RNN (toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a57354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 32)\n",
    "        self.rnn = nn.RNN(input_size=32, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, x, h=None):\n",
    "        x = self.embed(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        logits = self.fc(out)  # [B, T, V]\n",
    "        return logits, h\n",
    "\n",
    "# Dummy data: random characters\n",
    "vocab_size = 30\n",
    "model = CharRNN(vocab_size)\n",
    "x = torch.randint(0, vocab_size, (4, 20))  # batch=4, seq=20\n",
    "logits, h = model(x)\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e60112",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3) LSTM (Long Short-Term Memory)\n",
    "\n",
    "**Use when:** You need **longer context** than vanilla RNNs handle reliably (language modeling, time-series).\n",
    "\n",
    "### Intuition\n",
    "- LSTM introduces **gates** (input, forget, output) and a **cell state** to control information flow.\n",
    "\n",
    "### Equations\n",
    "Let $\\sigma$ be sigmoid and $\\odot$ element-wise multiplication.\n",
    "- $\\mathbf{i}_t = \\sigma(\\mathbf{W}_i [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i)$\n",
    "- $\\mathbf{f}_t = \\sigma(\\mathbf{W}_f [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f)$\n",
    "- $\\mathbf{g}_t = \\tanh(\\mathbf{W}_g [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_g)$\n",
    "- $\\mathbf{o}_t = \\sigma(\\mathbf{W}_o [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o)$\n",
    "- $\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\mathbf{g}_t$\n",
    "- $\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)$\n",
    "\n",
    "### Tips\n",
    "- Start with **GRU** if you want fewer parameters and similar performance.\n",
    "- Use **bidirectional** LSTMs for sequence tagging (NER, POS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb20e20",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch: LSTM for Sequence Classification (template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_size=128, num_layers=1, num_classes=2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        mult = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * mult, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        x = self.embed(x)\n",
    "        if lengths is not None:\n",
    "            # Pack padded sequences for efficiency on variable lengths\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            out, (h, c) = self.lstm(packed)\n",
    "            # h: [num_layers*dirs, B, H]\n",
    "            h_last = torch.cat([h[-2], h[-1]], dim=-1) if self.lstm.bidirectional else h[-1]\n",
    "        else:\n",
    "            out, (h, c) = self.lstm(x)\n",
    "            h_last = torch.cat([h[-2], h[-1]], dim=-1) if self.lstm.bidirectional else h[-1]\n",
    "        logits = self.fc(h_last)\n",
    "        return logits\n",
    "\n",
    "# Dummy data: token ids with padding id=0\n",
    "B, T, V = 8, 25, 1000\n",
    "x = torch.randint(1, V, (B, T))\n",
    "lengths = torch.randint(low=T//2, high=T, size=(B,))\n",
    "model = LSTMClassifier(vocab_size=V, bidirectional=True)\n",
    "logits = model(x, lengths=lengths)\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd76ac0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 4) Convolutional Neural Networks (CNN)\n",
    "\n",
    "**Use when:** Data have **spatial structure** (images, audio spectrograms), or local patterns matter.\n",
    "\n",
    "### Intuition\n",
    "- Convolutions apply **learnable local filters** across space/time.\n",
    "- Weight sharing makes CNNs parameter-efficient and translation-aware.\n",
    "\n",
    "### Key Concepts\n",
    "- **Kernel/Filter size**: receptive field size.\n",
    "- **Stride**: downsampling rate.\n",
    "- **Padding**: preserves spatial size.\n",
    "- **Pooling**: reduces resolution and adds invariance.\n",
    "\n",
    "### Output size (1D/2D)\n",
    "For input size $N$, kernel $K$, padding $P$, stride $S$:\n",
    "$$\n",
    "\\text{out} = \\left\\lfloor \\frac{N - K + 2P}{S} \\right\\rfloor + 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb73518",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch: Simple CNN for MNIST-like Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28->14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14->7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Dummy batch\n",
    "x = torch.randn(16, 1, 28, 28)\n",
    "model = SimpleCNN()\n",
    "logits = model(x)\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce50a7f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5) Transformers\n",
    "\n",
    "**Use when:** You need **long-range dependencies**, parallel training, and state-of-the-art performance (NLP, vision, multimodal).\n",
    "\n",
    "### Intuition\n",
    "- **Self-attention** lets each token attend to all others, modeling global context efficiently (quadratic in sequence length).\n",
    "- Add **positional encodings** to inject order information.\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "Given queries $Q$, keys $K$, values $V$:\n",
    "$$\n",
    "\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "### Multi-Head Attention (MHA)\n",
    "- Project inputs to multiple heads, apply attention in parallel, then concatenate.\n",
    "\n",
    "### Tips\n",
    "- Use **LayerNorm**, **residual connections**, and **dropout**.\n",
    "- **Causal masks** for autoregressive language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72758cc",
   "metadata": {},
   "source": [
    "\n",
    "### Minimal Self-Attention in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=128, n_heads=4):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)                       # [B,T,3C]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        # reshape to heads\n",
    "        def split_heads(t):\n",
    "            return t.view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # [B,H,T,D]\n",
    "        q, k, v = map(split_heads, (q, k, v))\n",
    "        # attention scores\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_head)  # [B,H,T,T]\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        out = attn @ v                                            # [B,H,T,D]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)      # [B,T,C]\n",
    "        return self.proj(out)\n",
    "\n",
    "# test\n",
    "x = torch.randn(2, 16, 128)  # batch, seq, features\n",
    "sa = SelfAttention(128, 4)\n",
    "y = sa(x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2b25e",
   "metadata": {},
   "source": [
    "\n",
    "### Transformer Encoder Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=128, n_heads=4, d_ff=256, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = SelfAttention(d_model, n_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm\n",
    "        x = x + self.dropout(self.attn(self.ln1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "# test block\n",
    "blk = TransformerEncoderBlock()\n",
    "y = blk(torch.randn(2, 16, 128))\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485db49",
   "metadata": {},
   "source": [
    "\n",
    "### Tiny Transformer Encoder for Text Classification (template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f179835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "class TinyTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=2, num_classes=2, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerEncoderBlock(d_model, n_heads, d_ff=4*d_model, p_drop=p_drop)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask=mask)\n",
    "        x = self.ln(x)\n",
    "        # Use mean pooling over sequence (CLS alternatives also fine)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Dummy batch of token ids\n",
    "B, T, V = 8, 32, 2000\n",
    "tokens = torch.randint(1, V, (B, T))\n",
    "model = TinyTransformerClassifier(vocab_size=V, num_classes=3)\n",
    "logits = model(tokens)\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5e4bc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Generic Training Loop (applies to all models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, dataloader, epochs=5, lr=1e-3, weight_decay=1e-4, max_grad_norm=1.0, device='cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total, correct, total_loss = 0, 0, 0.0\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb) if xb.ndim==2 else model(xb)  # image/text compatible\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "        print(f\"epoch {epoch}: loss={total_loss/total:.4f} acc={correct/total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc6e8c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Practical Guidance & Debugging Checklist\n",
    "\n",
    "- **Data sanity check**: Overfit a tiny subset (e.g., 100 samples). If it can't, something’s wrong (lr, bug, normalization).\n",
    "- **Learning rate**: Start with `1e-3` (Adam/AdamW); try lr range test or cosine schedules.\n",
    "- **Initialization**: Rely on PyTorch defaults; for deep MLPs, prefer `ReLU + He init`.\n",
    "- **Regularization**: Weight decay, dropout, early stopping. For sequences, also **label smoothing**.\n",
    "- **Batch size**: Increase until VRAM-bound; use gradient accumulation if needed.\n",
    "- **Mixed precision**: `torch.cuda.amp.autocast()` for speed on GPU.\n",
    "- **Monitoring**: Track loss/accuracy, learning rate, gradient norms, and activation stats.\n",
    "- **Reproducibility**: Set seeds and log versions of data & code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a981d3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Next Steps\n",
    "- Swap the dummy tensors with a real dataset using `torchvision` (for CNN) or `torchtext`/custom `Dataset` (for RNN/LSTM/Transformer).\n",
    "- Add **padding masks** for variable-length sequences in Transformers.\n",
    "- Try **GRU** variants and **Conv1D** for time-series.\n",
    "- For Transformers, explore **causal masking** for language modeling and **RoPE** for rotary positional encodings.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
