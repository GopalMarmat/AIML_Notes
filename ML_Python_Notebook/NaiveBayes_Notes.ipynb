{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Naive Bayes Classifier - In-Depth Notes\n", "\n", "Naive Bayes is a **probabilistic machine learning algorithm** used for classification tasks. It's based on **Bayes' Theorem** and assumes **feature independence**, which is why it's called \"naive\"."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Bayes' Theorem\n", "Bayes' Theorem allows us to calculate the probability of a class given the features:\n", "\n", "$$\n", "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n", "$$\n", "\n", "- $P(C|X)$: Posterior probability (class given data)\n", "- $P(X|C)$: Likelihood (data given class)\n", "- $P(C)$: Prior probability (probability of class)\n", "- $P(X)$: Evidence (probability of data)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Types of Naive Bayes Classifiers\n", "- **Gaussian Naive Bayes**: Assumes features follow a normal distribution.\n", "- **Multinomial Naive Bayes**: Used for discrete count features (e.g. word counts).\n", "- **Bernoulli Naive Bayes**: For binary/boolean features (e.g. 0/1 values)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example: Gaussian Naive Bayes on Iris Dataset\n", "from sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.metrics import accuracy_score, classification_report\n", "\n", "# Load dataset\n", "iris = load_iris()\n", "X, y = iris.data, iris.target\n", "\n", "# Split data\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "# Train model\n", "model = GaussianNB()\n", "model.fit(X_train, y_train)\n", "\n", "# Predict and evaluate\n", "y_pred = model.predict(X_test)\n", "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n", "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Assumption of Feature Independence\n", "Naive Bayes assumes that all features are **independent given the class label**. In practice, this assumption is rarely true, but the algorithm still performs well in many cases."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. When to Use Naive Bayes\n", "- Text classification (spam detection, sentiment analysis)\n", "- Recommendation systems\n", "- High-dimensional data problems\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Pros and Cons\n", "**Pros:**\n", "- Simple and fast\n", "- Works well with small datasets\n", "- Performs well on high-dimensional data (e.g. text)\n", "- Handles both binary and multiclass classification\n", "\n", "**Cons:**\n", "- Assumes independence between features\n", "- Struggles with correlated features\n", "- Not great for regression tasks"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example: Multinomial Naive Bayes for Text Classification\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.naive_bayes import MultinomialNB\n", "\n", "texts = [\"Free money now!!!\", \"Hi, how are you?\", \"Win big prizes\", \"Hello friend\"]\n", "labels = [1, 0, 1, 0]  # 1=spam, 0=not spam\n", "\n", "vectorizer = CountVectorizer()\n", "X = vectorizer.fit_transform(texts)\n", "\n", "clf = MultinomialNB()\n", "clf.fit(X, labels)\n", "pred = clf.predict(X)\n", "\n", "print(\"Predictions:\", pred)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}