{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23d0998",
   "metadata": {},
   "source": [
    "# ðŸ“˜ NLP Preprocessing Techniques\n",
    "\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Stopword Removal\n",
    "- Part-of-Speech (POS) Tagging\n",
    "- Word Embeddings (TF-IDF, Word2Vec, GloVe, Transformer embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a855681",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Tokenization\n",
    "Tokenization is the process of splitting text into smaller units (tokens), such as words, subwords, or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05566f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is fun to learn. It helps machines understand human language!\"\n",
    "\n",
    "# Word Tokenization\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "\n",
    "# Sentence Tokenization\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4058b9f",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Stemming\n",
    "Stemming reduces words to their root form by chopping suffixes. It may not produce valid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"flying\"]\n",
    "\n",
    "print([stemmer.stem(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036325e",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Lemmatization\n",
    "Lemmatization reduces words to their base or dictionary form (lemma), considering grammar and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"better\"]\n",
    "\n",
    "print([lemmatizer.lemmatize(w) for w in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39cb1df",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Stopword Removal\n",
    "Stopwords are common words (like *is, the, a, an*) that usually carry little meaning and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a75c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(\"This is an example showing off stopword filtration.\")\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"After Stopword Removal:\", filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cdd4f",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Part-of-Speech (POS) Tagging\n",
    "POS tagging assigns grammatical tags (noun, verb, adjective, etc.) to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ff0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = word_tokenize(\"John is learning NLP using Python.\")\n",
    "print(nltk.pos_tag(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f346dc8",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Word Embeddings\n",
    "Embeddings represent words as vectors of real numbers, capturing semantic meaning.\n",
    "### 1. TF-IDF Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Natural Language Processing with Python is fun\",\n",
    "    \"Deep learning advances NLP significantly\",\n",
    "    \"Python makes machine learning easier\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664ac0a",
   "metadata": {},
   "source": [
    "### 2. Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"nlp\", \"is\", \"fun\"], [\"python\", \"makes\", \"nlp\", \"easy\"]]\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=2)\n",
    "\n",
    "print(\"Vector for 'nlp':\", model.wv['nlp'])\n",
    "print(\"Most similar to 'nlp':\", model.wv.most_similar('nlp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed838a4f",
   "metadata": {},
   "source": [
    "### 3. spaCy Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65706b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # medium model with vectors\n",
    "doc = nlp(\"NLP is amazing with embeddings\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])  # print first 5 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e771e",
   "metadata": {},
   "source": [
    "### 4. Transformer-based Embeddings (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076da22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"NLP with Transformers is powerful\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(\"Embedding shape:\", outputs.last_hidden_state.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
