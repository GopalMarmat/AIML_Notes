{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee71c23b",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Notes with TensorFlow/Keras: ANN, RNN, CNN, LSTM, Transformers\n",
    "\n",
    "This notebook explains **ANN, RNN, CNN, LSTM, and Transformers** with intuition, equations, and **TensorFlow/Keras implementations**.\n",
    "\n",
    "> Requirements: `tensorflow`, `numpy`, `matplotlib` (install via `pip install tensorflow numpy matplotlib`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86707c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1. Artificial Neural Network (ANN / MLP)\n",
    "\n",
    "### Intuition\n",
    "- Dense layers with nonlinear activation functions (ReLU, GELU, etc.)\n",
    "- Use for tabular data or when features are pre-engineered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Simple ANN\n",
    "model_ann = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_ann.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_ann.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02223d63",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2. Recurrent Neural Network (RNN)\n",
    "\n",
    "### Intuition\n",
    "- Good for sequential data, but struggles with long dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bdcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple RNN for sequence classification\n",
    "model_rnn = keras.Sequential([\n",
    "    layers.Embedding(input_dim=5000, output_dim=32, input_length=100),\n",
    "    layers.SimpleRNN(64, return_sequences=False),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34263606",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "### Intuition\n",
    "- Handles long-term dependencies better than vanilla RNNs.\n",
    "- Commonly used for text and time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM model\n",
    "model_lstm = keras.Sequential([\n",
    "    layers.Embedding(input_dim=5000, output_dim=64, input_length=100),\n",
    "    layers.LSTM(128, return_sequences=False),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd3053",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 4. Convolutional Neural Network (CNN)\n",
    "\n",
    "### Intuition\n",
    "- Uses convolutional layers to capture spatial patterns in images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN model for MNIST-like images\n",
    "model_cnn = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e9a45",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5. Transformer (Self-Attention)\n",
    "\n",
    "### Intuition\n",
    "- Uses self-attention to capture global dependencies in sequences.\n",
    "- Backbone of models like BERT and GPT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16722521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple Transformer block using Keras\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Tiny Transformer for classification\n",
    "inputs = layers.Input(shape=(100, 64))\n",
    "x = TransformerBlock(64, 4, 128)(inputs)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model_transformer = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_transformer.summary()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
