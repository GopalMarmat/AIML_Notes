{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600c12cf",
   "metadata": {},
   "source": [
    "# üß† Recurrent Neural Networks (RNN) - In-depth Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32e7cc",
   "metadata": {},
   "source": [
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed for **sequential data**.\n",
    "They are widely used in Natural Language Processing (NLP), Time Series Forecasting, Speech Recognition, etc.\n",
    "\n",
    "Unlike traditional feedforward neural networks, RNNs have **loops** that allow information to persist, making them ideal for learning patterns in sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e231a",
   "metadata": {},
   "source": [
    "\n",
    "## üì¶ Applications of RNNs\n",
    "\n",
    "- Language Modeling and Text Generation\n",
    "- Sentiment Analysis\n",
    "- Machine Translation\n",
    "- Time Series Forecasting\n",
    "- Speech Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a3330",
   "metadata": {},
   "source": [
    "\n",
    "## üèóÔ∏è Basic RNN Architecture\n",
    "\n",
    "An RNN processes a sequence of inputs by maintaining a **hidden state** `h_t` which is updated at each time step:\n",
    "\n",
    "**Equations:**\n",
    "- Hidden state: \\( h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\)\n",
    "- Output: \\( y_t = W_{hy} h_t + b_y \\)\n",
    "\n",
    "Where:\n",
    "- \\( x_t \\): input at time t\n",
    "- \\( h_t \\): hidden state at time t\n",
    "- \\( W \\): weight matrices\n",
    "- \\( b \\): bias terms\n",
    "\n",
    "The output at each time step depends on the current input and previous hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1b7fc",
   "metadata": {},
   "source": [
    "\n",
    "### üîÅ Unrolled RNN (for 3 time steps)\n",
    "\n",
    "Each `( )` is an RNN cell sharing the same weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d06c3",
   "metadata": {},
   "source": [
    "## üîß Code Example: Simple RNN for Sentiment Classification (IMDb Dataset using Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e06bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load IMDb dataset\n",
    "vocab_size = 10000  # Only consider the top 10k words\n",
    "maxlen = 200  # Max length of each sequence\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=maxlen),\n",
    "    SimpleRNN(32),  # RNN Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d90c6",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ö†Ô∏è Limitations of Vanilla RNNs\n",
    "\n",
    "- **Vanishing/Exploding Gradients**: During backpropagation, gradients can become too small or too large, making training difficult.\n",
    "- **Short-term memory**: Struggles with long-term dependencies.\n",
    "- **Slow Training**: Sequential computation is less parallelizable.\n",
    "\n",
    "‚úÖ These problems are solved by advanced architectures like LSTM and GRU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70d3d8",
   "metadata": {},
   "source": [
    "\n",
    "## üîç Alternatives to Vanilla RNNs\n",
    "\n",
    "### 1. Long Short-Term Memory (LSTM)\n",
    "- Designed to combat vanishing gradient problem.\n",
    "- Uses **gates** (input, forget, output) to control flow of information.\n",
    "\n",
    "### 2. Gated Recurrent Unit (GRU)\n",
    "- A simpler version of LSTM with fewer gates (reset and update).\n",
    "- Efficient and performs comparably to LSTM on many tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9ed6c",
   "metadata": {},
   "source": [
    "## üîß Code Example: LSTM for IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b76bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# LSTM model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=maxlen),\n",
    "    LSTM(32),  # LSTM Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.summary()\n",
    "\n",
    "# Train model\n",
    "model_lstm.fit(x_train, y_train, epochs=2, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de22a5c",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Model | Memory | Complexity | Handles Long-term Dependency |\n",
    "|-------|--------|------------|-------------------------------|\n",
    "| RNN   | üü† Short | Low        | ‚ùå No                        |\n",
    "| LSTM  | üü¢ Long  | High       | ‚úÖ Yes                       |\n",
    "| GRU   | üü¢ Long  | Medium     | ‚úÖ Yes                       |\n",
    "\n",
    "RNNs are foundational in sequence modeling. However, for most practical applications today, **LSTM or GRU** are preferred.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
