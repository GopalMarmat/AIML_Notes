{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Support Vector Machine (SVM) - In-Depth Notes\n", "\n", "Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. SVMs are particularly effective in high-dimensional spaces and when the number of dimensions exceeds the number of samples.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Core Idea\n", "SVM tries to find the optimal hyperplane that best separates the data into different classes. For linearly separable data, it finds the hyperplane that maximizes the margin between two classes.\n", "\n", "**Key Terms:**\n", "- **Hyperplane**: A decision boundary that separates different classes.\n", "- **Margin**: Distance between the hyperplane and the nearest data points from each class.\n", "- **Support Vectors**: Data points that lie closest to the hyperplane and influence its position and orientation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sample illustration of linear SVM\n", "from sklearn.datasets import make_blobs\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "from sklearn.svm import SVC\n", "\n", "# Create a sample dataset\n", "X, y = make_blobs(n_samples=50, centers=2, random_state=6)\n", "clf = SVC(kernel='linear')\n", "clf.fit(X, y)\n", "\n", "# Plot\n", "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='bwr')\n", "ax = plt.gca()\n", "xlim = ax.get_xlim()\n", "ylim = ax.get_ylim()\n", "\n", "# Create grid to evaluate model\n", "xx = np.linspace(xlim[0], xlim[1])\n", "yy = np.linspace(ylim[0], ylim[1])\n", "YY, XX = np.meshgrid(yy, xx)\n", "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n", "Z = clf.decision_function(xy).reshape(XX.shape)\n", "\n", "# Plot decision boundary and margins\n", "plt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n", "            linestyles=['--', '-', '--'])\n", "\n", "# Plot support vectors\n", "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n", "            linewidth=1, facecolors='none', edgecolors='k')\n", "plt.title(\"SVM with Linear Kernel\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Kernel Trick\n", "For non-linearly separable data, SVM uses a technique called the **kernel trick** to transform data into a higher-dimensional space where it becomes linearly separable.\n", "\n", "**Popular Kernels:**\n", "- Linear\n", "- Polynomial\n", "- Radial Basis Function (RBF)\n", "- Sigmoid"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example with non-linear kernel (RBF)\n", "from sklearn.datasets import make_circles\n", "\n", "X, y = make_circles(n_samples=100, factor=.1, noise=.1)\n", "clf = SVC(kernel='rbf', C=1)\n", "clf.fit(X, y)\n", "\n", "# Plot\n", "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='bwr')\n", "ax = plt.gca()\n", "xlim = ax.get_xlim()\n", "ylim = ax.get_ylim()\n", "\n", "xx = np.linspace(xlim[0], xlim[1])\n", "yy = np.linspace(ylim[0], ylim[1])\n", "YY, XX = np.meshgrid(yy, xx)\n", "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n", "Z = clf.decision_function(xy).reshape(XX.shape)\n", "\n", "plt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n", "            linestyles=['--', '-', '--'])\n", "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n", "            linewidth=1, facecolors='none', edgecolors='k')\n", "plt.title(\"SVM with RBF Kernel\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Important Parameters\n", "- `C`: Regularization parameter. A small `C` makes the margin wider but allows misclassification. A large `C` tries to classify all training examples correctly.\n", "- `kernel`: Specifies the kernel type (`linear`, `poly`, `rbf`, `sigmoid`).\n", "- `gamma`: Kernel coefficient for `rbf`, `poly`, and `sigmoid`. Defines how far the influence of a single training example reaches.\n", "- `degree`: Degree of the polynomial kernel function (`poly`)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Pros and Cons\n", "**Pros:**\n", "- Effective in high-dimensional spaces.\n", "- Memory efficient (uses support vectors).\n", "- Versatile with different kernel functions.\n", "\n", "**Cons:**\n", "- Not suitable for large datasets (high training time).\n", "- Choosing the right kernel and parameters can be complex.\n", "- Less effective when data is heavily noisy and overlapping."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}