{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Gradient Boosting vs AdaBoost\n", "\n", "This notebook provides an in-depth explanation of **Gradient Boosting** and **AdaBoost** with examples, comparisons with **Random Forest**, and decision tree diagrams for each method.\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udf33 What is Boosting?\n", "Boosting is an **ensemble technique** that combines multiple weak learners to form a strong learner. It builds models sequentially such that each new model corrects the errors of the previous ones.\n", "\n", "- **Weak Learner**: A model that performs slightly better than random guessing (e.g., decision stump).\n", "- **Strong Learner**: Combined model that achieves high accuracy.\n", "\n", "**Key Boosting Techniques:**\n", "- AdaBoost\n", "- Gradient Boosting\n", "- XGBoost / LightGBM / CatBoost (advanced variants)\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd01 AdaBoost (Adaptive Boosting)\n", "### Concept:\n", "- Focuses on misclassified data points by assigning them higher weights.\n", "- Trains a sequence of weak learners, where each learner tries to fix the mistakes of the previous one.\n", "\n", "### Key Points:\n", "- Uses **decision stumps** (1-level trees).\n", "- Each model gets a **weight** based on its performance.\n", "- Combines models using weighted majority vote (classification) or weighted sum (regression).\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# AdaBoost Example\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.datasets import make_classification\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "\n", "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "\n", "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n", "ada.fit(X_train, y_train)\n", "y_pred = ada.predict(X_test)\n", "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udfaf Gradient Boosting\n", "### Concept:\n", "- Optimizes a loss function by adding weak learners in a **stage-wise fashion**.\n", "- Each new model is trained on the **residual errors** of the previous model.\n", "\n", "### Key Points:\n", "- Learns **gradients of the loss function**.\n", "- Supports both regression and classification.\n", "- Slower but more **accurate** than AdaBoost.\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Gradient Boosting Example\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "\n", "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n", "gb.fit(X_train, y_train)\n", "y_pred_gb = gb.predict(X_test)\n", "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd0d Difference Between AdaBoost, Gradient Boosting and Random Forest\n", "\n", "| Feature | AdaBoost | Gradient Boosting | Random Forest |\n", "|--------|----------|------------------|----------------|\n", "| Learner | Sequential | Sequential | Parallel |\n", "| Weak Learner | Decision Stumps | Decision Trees | Full Trees |\n", "| Focus | Misclassified samples | Residual errors | Bootstrap aggregation |\n", "| Optimization | Exponential loss | Any differentiable loss | Not required |\n", "| Overfitting | Less prone | Can overfit | Less prone |\n", "| Speed | Fast | Slower | Fast |\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udf32 Visualizing Tree for AdaBoost and Gradient Boosting\n", "Let's visualize the first tree for both AdaBoost and Gradient Boosting to understand the difference."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.tree import plot_tree\n", "import matplotlib.pyplot as plt\n", "\n", "# AdaBoost First Tree\n", "plt.figure(figsize=(12, 6))\n", "plot_tree(ada.estimators_[0], filled=True)\n", "plt.title(\"AdaBoost - First Tree\")\n", "plt.show()\n", "\n", "# Gradient Boosting First Tree\n", "plt.figure(figsize=(12, 6))\n", "plot_tree(gb.estimators_[0, 0], filled=True)\n", "plt.title(\"Gradient Boosting - First Tree\")\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}