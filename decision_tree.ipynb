{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udf33 Decision Tree - In-depth Notes\n", "\n", "Decision Trees are a **supervised learning** algorithm used for both **classification and regression** tasks. They split data into subsets based on feature values to create a tree structure."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd11 1. Key Concepts\n", "- **Root Node**: The first split (most informative feature)\n", "- **Internal Nodes**: Features that split data\n", "- **Leaf Nodes**: Final output labels or values\n", "- **Splitting**: Dividing dataset based on feature\n", "- **Pruning**: Reducing tree size to avoid overfitting"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83e\udde0 2. Impurity Measures\n", "- **Gini Impurity**:\n", "$$\n", "G = 1 - \\sum p_i^2\n", "$$\n", "- **Entropy**:\n", "$$\n", "H = - \\sum p_i \\log_2(p_i)\n", "$$\n", "\n", "Both are used to evaluate the quality of a split. Lower value = purer node."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2699\ufe0f 3. Implementing Decision Tree Classifier (Scikit-learn)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_iris\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import classification_report, accuracy_score\n", "\n", "# Load Data\n", "data = load_iris()\n", "X = data.data\n", "y = data.target\n", "\n", "# Split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n", "\n", "# Train\n", "model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n", "model.fit(X_train, y_train)\n", "\n", "# Predict\n", "y_pred = model.predict(X_test)\n", "print(classification_report(y_test, y_pred))\n", "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udf33 4. Visualizing the Tree"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.tree import plot_tree\n", "import matplotlib.pyplot as plt\n", "\n", "plt.figure(figsize=(12,8))\n", "plot_tree(model, filled=True, feature_names=data.feature_names, class_names=data.target_names)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u2702\ufe0f 5. Pruning to Prevent Overfitting\n", "- **max_depth**: Limit the depth\n", "- **min_samples_split**: Minimum samples to split node\n", "- **min_samples_leaf**: Minimum samples at leaf\n", "\n", "These are controlled using hyperparameters in `DecisionTreeClassifier()`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd0d 6. Feature Importance\n", "Decision Trees provide a measure of **feature importance** based on how much each feature decreases impurity."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "importance = pd.Series(model.feature_importances_, index=data.feature_names)\n", "importance.sort_values(ascending=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udccc Summary\n", "- Used for both regression and classification\n", "- Easy to visualize and interpret\n", "- Prone to overfitting (use pruning)\n", "- Scikit-learn makes implementation simple"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}